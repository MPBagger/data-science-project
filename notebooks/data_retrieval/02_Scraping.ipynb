{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Crawling and web Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes, they have APIs but they have no well-written packages in the language you prefer (e.g. only Java but no Python libraries). Even worse, there may not be APIs for the public and we have to design a scraper to retrieve all the relevant informaiton we want. In such cases, we can manually build our own wrapper functions.\n",
    "\n",
    "Web crawling and web scraping are two related techniques used to extract information from websites.\n",
    "\n",
    "Web crawling, also known as web indexing or web spidering, is the process of automatically exploring and indexing web pages on the internet. Web crawlers, also called spiders, bots, or robots, navigate through websites, follow links, and index the content of the pages they encounter. Search engines like Google and Bing use web crawlers to build their indexes of web pages, which enables users to find information easily.\n",
    "\n",
    "Web scraping, on the other hand, is the process of extracting specific data from web pages. Web scraping involves analyzing the HTML structure of a webpage, identifying the relevant information, and extracting it into a structured format such as a CSV or JSON file. Web scraping can be used to extract product information, pricing data, news articles, and more.\n",
    "\n",
    "Web crawling and web scraping can be done manually, but it's often more efficient to use specialized software tools. Python is a popular language for web crawling and web scraping, and there are many libraries available, including BeautifulSoup, Scrapy, and Selenium.\n",
    "\n",
    "However, it's important to note that web scraping can raise legal and ethical concerns, particularly if done without permission or in violation of website terms of service. Web scraping can also put a strain on website servers, potentially causing them to crash or become unavailable. As such, it's important to use web scraping responsibly and within legal and ethical boundaries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Preliminiary examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examples from <a href=\"https://www.w3schools.com/html/tryit.asp?filename=tryhtml_basic_document\" target=\"blank_\">w3schools</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```html\n",
    "<!DOCTYPE html>\n",
    "<html>\n",
    "<body>\n",
    "\n",
    "<h1>My First Heading</h1>\n",
    "\n",
    "<p>My 1st paragraph.</p>\n",
    "<p>My 2nd paragraph.</p>\n",
    "<p>My 3rd paragraph.</p>\n",
    "\n",
    "</body>\n",
    "</html>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save this code to your disk as `sample.html` (or any other name). We will use a great library called ___`Beautiful Soup`___ to read the contents from Python. You may also need to install lxml, which is for parsing specific formats (e.g., html and xml).\n",
    "\n",
    "    poetry add beautifulsoup4 lxml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-15T00:59:26.608656Z",
     "start_time": "2019-03-15T00:59:26.448596Z"
    }
   },
   "outputs": [],
   "source": [
    "## Do the following if you have not\n",
    "\n",
    "from bs4 import BeautifulSoup as Soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-15T00:59:26.766935Z",
     "start_time": "2019-03-15T00:59:26.761404Z"
    }
   },
   "outputs": [],
   "source": [
    "with open(\"data/sample.html\", \"r\") as sample:\n",
    "    sample_contents = sample.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The structure of HTML is not displayed properly without BeautifulSoup, which is really hand!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-15T00:59:28.135370Z",
     "start_time": "2019-03-15T00:59:28.123466Z"
    }
   },
   "outputs": [],
   "source": [
    "sample_contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(sample_contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-15T00:59:28.612880Z",
     "start_time": "2019-03-15T00:59:28.604759Z"
    }
   },
   "outputs": [],
   "source": [
    "sample_soup = Soup(sample_contents, 'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(sample_soup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By printing it, we can see the exact contents as shown above with proper indentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-15T00:59:29.159746Z",
     "start_time": "2019-03-15T00:59:29.155620Z"
    }
   },
   "outputs": [],
   "source": [
    "print(sample_soup.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the contents of interest: all the `p`'s\n",
    "\n",
    "_`p` means paragraph in html. Check more tag definitions on [w3schools.org](https://www.w3schools.com/tags/default.asp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-15T00:59:30.637396Z",
     "start_time": "2019-03-15T00:59:30.634466Z"
    }
   },
   "outputs": [],
   "source": [
    "p_tags = sample_soup.find_all(\"p\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-15T00:59:30.973087Z",
     "start_time": "2019-03-15T00:59:30.965799Z"
    }
   },
   "outputs": [],
   "source": [
    "p_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-15T00:59:31.195724Z",
     "start_time": "2019-03-15T00:59:31.187710Z"
    }
   },
   "outputs": [],
   "source": [
    "p_tags[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(p_tags[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each of the `p` tag, we get the textual value out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-15T00:59:32.440325Z",
     "start_time": "2019-03-15T00:59:32.436504Z"
    }
   },
   "outputs": [],
   "source": [
    "for p in p_tags:\n",
    "    print(p.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A real example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use a real website for illustration. For example, if we are interested in the danish parliments webpage for handeling citizen proposals [borgerforslag](https://www.borgerforslag.dk)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To view the \"text style\" or the real structure of a web page, you can use ___`developer tools`___ function in your browser."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that [`requests`](http://docs.python-requests.org/) is a convenient package for sending HTTP requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-15T01:00:07.490430Z",
     "start_time": "2019-03-15T01:00:07.365897Z"
    }
   },
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-15T01:03:09.166347Z",
     "start_time": "2019-03-15T01:03:08.641354Z"
    }
   },
   "outputs": [],
   "source": [
    "borger_url = \"https://www.borgerforslag.dk/se-og-stoet-forslag/?Id=FT-14316\"\n",
    "r = requests.get(borger_url)\n",
    "r.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-15T01:05:54.889840Z",
     "start_time": "2019-03-15T01:05:54.881016Z"
    }
   },
   "outputs": [],
   "source": [
    "r.text[300:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-03T04:20:18.387485Z",
     "start_time": "2018-03-03T04:20:18.356444Z"
    }
   },
   "source": [
    "Convert it to a soup object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-15T01:05:56.532482Z",
     "start_time": "2019-03-15T01:05:56.495219Z"
    }
   },
   "outputs": [],
   "source": [
    "borger_soup = Soup(r.text, 'lxml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the correponding tag. Note that `class_` has a trailing underscore `_`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "borger_soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_tag = borger_soup.find_all('div')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-15T01:12:12.262796Z",
     "start_time": "2019-03-15T01:12:12.246056Z"
    }
   },
   "outputs": [],
   "source": [
    "summary_tag = borger_soup.find('div', class_='article')\n",
    "\n",
    "print('--------------------')\n",
    "print(summary_tag)\n",
    "print('--------------------')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "borger_content = summary_tag.contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "borger_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(borger_content[2].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this in mind, you can scrape almost any webpage of interest. Other formats such as <a href=\"http://www.json.org/\" target=\"_blank\">JSON</a> and <a href=\"https://www.w3.org/XML/\" target=\"_blank\">XML</a> do have high similarities and a few differences. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***But keep in mind that you should act politely, with propoer permission!! To find out whether specific paths/contents are allowed to be scraped, you can check their ___`robots.txt`___. For example, <a href=\"https://www.google.com/robots.txt\" target=\"_blank\">here's</a> the permission information set by Google.***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the examples we are using here are relatively simple. There are cases that we cannot access the pagination/scoll simply by `requests` alone. In those cases, [Selenium](http://selenium-python.readthedocs.io/) will save our lifes by ___simulating Browsers___!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some more tutorials/tools:\n",
    "\n",
    "- https://scrapy.org/ #building a crawler \n",
    "- https://www.dataquest.io/blog/web-scraping-tutorial-python/\n",
    "- https://www.quora.com/Python-programming-language-1/How-is-BeautifulSoup-different-from-Scrapy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "return to [overview](../00_overview.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "133px",
    "width": "254px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
