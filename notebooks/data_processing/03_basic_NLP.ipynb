{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OfKt9ernTGcK"
   },
   "source": [
    "# Basic Natural Language Processing (NLP)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tH21OrjWooPS",
    "outputId": "9cd931cc-c520-4a03-88aa-f9a3de01960a"
   },
   "source": [
    "The goal of NLP is to bridge the gap between human communication and computer understanding. It is used in a wide range of applications such as text classification, sentiment analysis, machine translation, chatbots, and more.\n",
    "\n",
    "One of the main steps in NLP is transforming text data into a structured format that can be used for analysis. This is often done by converting the text into a tabular format. The main steps involved in this transformation are:\n",
    "\n",
    "- Tokenization: Breaking the text down into smaller units, such as words or phrases, known as tokens.\n",
    "\n",
    "- Whitespace and delimiter removal: Removing whitspaces and delimiters from the text or list og tokens.\n",
    "\n",
    "- Stop word removal: Removing commonly used words that do not carry much meaning, such as \"the\", \"a\", \"an\", etc.\n",
    "\n",
    "- Stemming or Lemmatization: Reducing words to their root form, to simplify the analysis.\n",
    "\n",
    "- Part-of-speech (POS) tagging: Identifying the grammatical role of each word in a sentence.\n",
    "\n",
    "- Named entity recognition (NER): Identifying and classifying entities in the text, such as people, organizations, and locations.\n",
    "\n",
    "- Creating a table: The final step is to organize the extracted information into a structured format, such as a spreadsheet or database table. The rows of the table represent individual documents, while the columns represent the extracted features, such as words or entities.\n",
    "\n",
    "Overall, transforming text into a tabular format is a crucial step in NLP, as it enables the application of a wide range of analytical techniques to better understand and make use of the data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Document representation\n",
    "\n",
    "![tabular](../images/text_tabular.png)\n",
    "\n",
    "\n",
    "While we as human can understand the text, machines can hardly know text as it is. A classic way to mathematically represent text data is to convert them into [vector spaces](https://en.wikipedia.org/wiki/Vector_space_model). Specifically, each documeent will be stored as a vector, where each element is a ___weight___ for one term. A simple and commonly used approach is to tokenize documents and use term frequencies or [term frequency-inverse document frequency (TFIDF)](https://en.wikipedia.org/wiki/Tf%E2%80%93idf) as weights.\n",
    "\n",
    "\n",
    "There are various representations of text in tabular form\n",
    "\n",
    "- **binary (0,1)**: present or not\n",
    "- **frequency**: count or proportion\n",
    "- **(0,1,2)**: present zero, one or more times.\n",
    "- **tf-idf**: \n",
    "$$tf-idf_j = tf_j * log( \\frac{N}{df_j})$$\n",
    "\n",
    "                                                j: specific term\n",
    "                                                tf: term frequency (in a document)\n",
    "                                                df: document frequency (frequency of documents with the term)\n",
    "                                                N: number of documents\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Definitions\n",
    "\n",
    "- **corpus**: collection of text documents\n",
    "- **tokens**: words, numbers, acronyms, symbols, fixed-length character strings etc. (in a document, corpus etc.)\n",
    "- **types**: the possible (distinct) words in a document, corpus etc.\n",
    "- **delimiter**: “.”, “,”, “!” etc. (can be tokens to)\n",
    "- **whitespace**: “ “, “\\n”,  “\\t“\n",
    "- **dictionary**: collection of words mapping unto e.g. definitions, synonyms etc.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We will be using the library [nltk](https://www.nltk.org/) for working with natural language.\n",
    "\n",
    "`pip install nltk`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "aS5aFgDKqOBI"
   },
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag-of-Words (BOW)\n",
    "While more sophisticated approaches are available, BOW representation is still very popular due to its simplicity. In this style, we can forget word orders and treat each document as ___a bag of words___. While this is a very strong assumption, it still makes sense -- we can understand a sentence even if the words are randomly ordered. For example, we can easily understand the following sentence:\n",
    "\n",
    "> sitting a chair is cat there on"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Python, we can use a `numpy.ndarray` or `list` to save this information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sitting', 'a', 'chair', 'is', 'cat', 'there', 'on']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = ['sitting', 'a', 'chair', 'is', 'cat', 'there', 'on']\n",
    "sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_4JpHHRXTKrI"
   },
   "source": [
    "## Tokenization\n",
    "\n",
    "Tokenization is the process of splitting a text into smaller units called tokens. These tokens are usually words or sub-words, although they can also be punctuation marks, numbers, or other meaningful units of a text.\n",
    "\n",
    "In English text, we can simply split each document by spaces. Besides, we usually may want to remove puncutations because they do not provide valuable information. Last but not least, we may want to exclude some very common words (e.g., `the`, `we`, `you`, `is`, etc.), called [___stop words___](https://en.wikipedia.org/wiki/Stop_words)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3PwgHiH5qTQs",
    "outputId": "f85a9161-e631-475c-a231-d4b281299543"
   },
   "outputs": [],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"The first step in handling text is to break the stream \\\n",
    "of characters into words or, precisely, tokens. This is fundamental \\\n",
    "to further analysis. Without identifying the tokens, it is difficult \\\n",
    "to imagine extracting higher-level information from the document. \\\n",
    "Each token is an instance of a type, so the number of tokens is much \\\n",
    "higher than the number of types\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kZAtuBOHr0Hi",
    "outputId": "7fc051fd-d5f0-4202-b1c5-6bf1816797a6"
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "tokens = word_tokenize(text.lower())\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YZavEyf3shdA",
    "outputId": "a0bfa192-f1c1-4f10-ab67-f2520a424b20"
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "sentence = sent_tokenize(text)\n",
    "print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Delimiter/punctuation removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out the punctuation marks\n",
    "tokens = [token for token in tokens if token not in string.punctuation]\n",
    "\n",
    "print(tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ug23r1ziTRBB"
   },
   "source": [
    "#### Stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6J_X7WZCtvSO",
    "outputId": "f9483e7a-aaa1-4531-db9d-6a6ebbdaa70f"
   },
   "outputs": [],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NkFPhwmAt0_m",
    "outputId": "d404217a-abd2-4f13-98bf-5734317d084c"
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stopwords = stopwords.words(\"english\")\n",
    "print(stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = [token for token in tokens if token not in stopwords ]\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KPDmzpqRTUdn"
   },
   "source": [
    "## Stemming and Lemmatization\n",
    "\n",
    "Stemming and lemmatization are two common techniques used in Natural Language Processing (NLP) to reduce words to their base or root form. Both of these techniques are used to normalize words and reduce the dimensionality of the feature space, which can improve the performance of text classification and information retrieval models.\n",
    "\n",
    "- **Stemming** is the process of reducing a word to its base form by removing the suffixes, prefixes, or inflectional endings of the word. The resulting word, called a stem, may not be a valid word in the language, but it represents the root of the original word. For example, the word \"jumping\" would be stemmed to \"jump\", and the word \"cats\" would be stemmed to \"cat\".\n",
    "\n",
    "- **Lemmatization** is the process of reducing a word to its base form, called a lemma, by using the context and part of speech of the word. The resulting lemma is a valid word in the language and represents the base meaning of the original word. For example, the word \"jumping\" would be lemmatized to \"jump\", and the word \"cats\" would be lemmatized to \"cat\".\n",
    "\n",
    "The main difference between stemming and lemmatization is that stemming is a heuristic, rule-based process that simply removes the suffixes or prefixes of a word to obtain the stem, whereas lemmatization uses more complex rules and the context of the word to determine the base form of the word.\n",
    "\n",
    "Suppose we have the sentence \"The cats were jumping over the fences\". After tokenizing the sentence into individual words, we can apply stemming and lemmatization to each word:\n",
    "\n",
    "- Stemming: The stems of the words in the sentence would be \"the\", \"cat\", \"were\", \"jump\", \"over\", \"the\", and \"fenc\". As you can see, some of the resulting stems, such as \"fenc\", are not valid words.\n",
    "- Lemmatization: The lemmas of the words in the sentence would be \"the\", \"cat\", \"be\", \"jump\", \"over\", \"the\", and \"fence\". All of the resulting lemmas are valid words, and the verb \"be\" is correctly lemmatized to \"be\" instead of \"were\".\n",
    "\n",
    "In general, lemmatization tends to produce more accurate results than stemming, but it can be slower and more computationally expensive. The choice between stemming and lemmatization depends on the specific application and the trade-off between accuracy and efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "st2JbM81u0RJ",
    "outputId": "b7c145b7-7e74-43b9-ba44-00605addf2fe"
   },
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "ps = PorterStemmer()\n",
    "\n",
    "stems = [ps.stem(token) for token in tokens]\n",
    "print(stems)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import these modules\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "  \n",
    "lemmatizer = WordNetLemmatizer()\n",
    "  \n",
    "lemma = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "print(lemma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kBjGg2nrTYAm"
   },
   "source": [
    "## Counting Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1WchynDMVnu8",
    "outputId": "963e44a7-c333-4c17-e414-2e14dc7e1376"
   },
   "outputs": [],
   "source": [
    "FreqDist = nltk.FreqDist(tokens)\n",
    "for i,j in FreqDist.items():\n",
    "    if j > 1:\n",
    "        print(i, \"---\", j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FreqDist = nltk.FreqDist(lemma)\n",
    "for i,j in FreqDist.items():\n",
    "    if j > 1:\n",
    "        print(i, \"---\", j)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J6au6fb1Tilk"
   },
   "source": [
    "## Word groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7eWR25NzQDtv",
    "outputId": "fb7de023-b3ed-4ee4-8ffa-da5ffa45a826"
   },
   "outputs": [],
   "source": [
    "# Bigrams\n",
    "print(list(nltk.bigrams(tokens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GzrbKs2jRFfP",
    "outputId": "7fe0bbfe-2c7c-465b-b988-0a26f94f05e8"
   },
   "outputs": [],
   "source": [
    "# Trigrams\n",
    "print(list(nltk.trigrams(tokens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fj1BCGX_RX_J",
    "outputId": "f6ad9d69-58c9-4826-e139-ce0f9a5eca5f"
   },
   "outputs": [],
   "source": [
    "# N-grams\n",
    "print(list(nltk.ngrams(tokens, 4)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nd30v8wvS8cM"
   },
   "source": [
    "## POS Taggers\n",
    "\n",
    "POS (Part-of-Speech) tagging, also known as grammatical tagging, is the process of labeling each word in a text with its corresponding part of speech, such as noun, verb, adjective, or adverb. \n",
    "\n",
    "[POS Tagging](https://www.nltk.org/book/ch05.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "K_Z59DxgTCaD",
    "outputId": "0f7a507b-bf6e-4ffa-ec75-279979468ae9"
   },
   "outputs": [],
   "source": [
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "pos = nltk.pos_tag(lemma)\n",
    "print(pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9vGnMzBTVAn6"
   },
   "source": [
    "## Named Entity Recognition\n",
    "\n",
    "Named Entity Recognition (NER) involves identifying and categorizing named entities in a text into predefined categories such as people, organizations, locations, products, or dates. Named entities are typically proper nouns or noun phrases that refer to specific entities in the real world."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6hAhTQJaU--_",
    "outputId": "9d2d1fae-b5a6-4841-84f5-29f8f8e0b829"
   },
   "outputs": [],
   "source": [
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X1NpKVSHWSfj",
    "outputId": "d5c83b38-882e-4591-b207-da77e8224ffe"
   },
   "outputs": [],
   "source": [
    "Text = \"The russian president Vladimir Putin is in the Kremlin\"\n",
    "Tokenize = nltk.word_tokenize(Text)\n",
    "POS_tags = nltk.pos_tag(Tokenize)\n",
    "NameEn = nltk.ne_chunk(POS_tags)\n",
    "print(NameEn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We ca also use the library [spacy](https://spacy.io/api/data-formats#section-named-entities).\n",
    "\n",
    "install spacy `pip install spacy` and download the model [en_core_web_sm](https://spacy.io/models/en) `python -m spacy download en_core_web_sm`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "from collections import Counter\n",
    "import en_core_web_sm\n",
    "\n",
    "nlp = en_core_web_sm.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(Text)\n",
    "print([(X.text, X.label_) for X in doc.ents])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text in tabular form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/anmeldelser.txt', 'r') as file:\n",
    "    anmeldelser = file.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anmeldelser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "stopwords = stopwords.words(\"danish\")\n",
    "\n",
    "nltk.download('wordnet')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def tokenize(text: str, remove_num: bool = True, custom_remove: list = ['\"', '``', \"''\"]) -> list:\n",
    "    \"\"\"\n",
    "    Tokenizes and lemmatizes the input text.\n",
    "\n",
    "    Args:\n",
    "        text (str): The input text to tokenize and lemmatize.\n",
    "        remove_num (bool): Whether to remove numbers from the text. Defaults to True.\n",
    "        custom_remove (list): List of custom tokens to remove from the text. Defaults to ['\"', '``', \"''\"].\n",
    "\n",
    "    Returns:\n",
    "        list: A list of lemmatized tokens.\n",
    "\n",
    "    Example:\n",
    "        >>> text = \"This is an example sentence with punctuation, numbers 123, and stop words.\"\n",
    "        >>> tokenize(text)\n",
    "        ['example', 'sentence', 'punctuation', 'number', 'stop', 'word']\n",
    "    \"\"\"\n",
    "    if remove_num:\n",
    "        text = re.sub(r'[0-9]+', '', text)\n",
    "    tokens = [token.lower() for token in word_tokenize(text)]\n",
    "    tokens = [token for token in tokens if token not in stopwords \n",
    "              and token not in string.punctuation and token not in custom_remove]        \n",
    "    lemmatized = [lemmatizer.lemmatize(item) for item in tokens]\n",
    "    return lemmatized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a TFIDF object, applying some settings\n",
    "tfidf = TfidfVectorizer(analyzer='word',\n",
    "                        sublinear_tf=True,\n",
    "                        max_features=5000,\n",
    "                        tokenizer=tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TD-IDF Matrix\n",
    "X = tfidf.fit_transform(anmeldelser)\n",
    "\n",
    "# extracting feature names\n",
    "tfidf_tokens = tfidf.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "result = pd.DataFrame(\n",
    "    data=X.toarray(), \n",
    "    columns=tfidf_tokens\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AG2gqbhMZqP8"
   },
   "source": [
    "return to [overview](../00_overview.ipynb)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
