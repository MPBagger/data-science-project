{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d1acd617-c093-4406-9ea8-61289e29e12c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Data cleaning\n",
    "\n",
    "Data cleaning is a crucial step in the data analysis process, which involves identifying and correcting errors, inconsistencies, and discrepancies in the data. The main steps involved in data cleaning are as follows:\n",
    "\n",
    "- **Identify missing values**: The first step in data cleaning is to identify any missing values in the data. Missing values can arise due to a variety of reasons, such as incomplete data collection, errors in data entry, or data corruption. Identifying and handling missing values is important as they can affect the results of the analysis.\n",
    "\n",
    "- **Remove duplicates**: Duplicate data points can occur due to errors in data collection or data entry. Duplicate data can distort the analysis and lead to incorrect results. Identifying and removing duplicates is an important step in data cleaning.\n",
    "\n",
    "- **Correct inconsistent data**: Inconsistent data can arise due to errors in data entry, data corruption, or data integration from multiple sources. Inconsistent data can include spelling errors, numerical errors, or discrepancies in the format of data. Correcting inconsistent data involves identifying the errors and making the necessary corrections.\n",
    "\n",
    "- **Standardize data**: Standardizing data involves converting data into a consistent format. For example, converting all dates into a standard format or converting all text to lowercase. Standardizing data is important for analysis as it allows for easier comparison and analysis.\n",
    "\n",
    "- **Handle outliers**: Outliers are data points that are significantly different from the other data points. Outliers can arise due to errors in data collection, data corruption, or genuine differences in the data. Handling outliers involves identifying the outliers and deciding how to handle them. Outliers can be removed or treated differently in the analysis.\n",
    "\n",
    "- **Validate data**: Validating data involves checking the data for accuracy and consistency. This involves checking the data against known values or sources of information to ensure that it is accurate and consistent.\n",
    "\n",
    "Overall, data cleaning is an iterative process that involves identifying and correcting errors in the data until the data is clean and ready for analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2953cf1d",
   "metadata": {},
   "source": [
    "### Titanic\n",
    "\n",
    "The Titanic dataset is a famous dataset used in the field of data science and machine learning. It contains information about the passengers who were aboard the RMS Titanic when it sank on its maiden voyage in April 1912. The dataset is often used as an introductory dataset for learning data analysis and machine learning algorithms.\n",
    "\n",
    "The Titanic dataset contains the following information for each passenger:\n",
    "\n",
    "- PassengerId: A unique identifier for each passenger\n",
    "- Survived: A binary variable indicating whether the passenger survived (1) or not (0)\n",
    "- Pclass: The passenger's class (1st, 2nd, or 3rd)\n",
    "- Name: The passenger's name\n",
    "- Sex: The passenger's gender\n",
    "- Age: The passenger's age\n",
    "- SibSp: The number of siblings or spouses the passenger had aboard the Titanic\n",
    "- Parch: The number of parents or children the passenger had aboard the Titanic\n",
    "- Ticket: The passenger's ticket number\n",
    "- Fare: The fare paid by the passenger\n",
    "- Cabin: The passenger's cabin number\n",
    "- Embarked: The port where the passenger embarked (C = Cherbourg, Q = Queenstown, S = Southampton)\n",
    "\n",
    "The original dataset contains a total of 891 rows (here 894 rows), corresponding to the number of passengers in the dataset. The Survived column is the target variable, and the other columns are used as features for predicting whether a passenger survived or not. The dataset is often used to build predictive models to determine which passengers were more likely to survive the sinking of the Titanic based on their characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a3bdc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "titanic = pd.read_csv('data/titanic_with_dub.csv')\n",
    "titanic.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c26a7b18",
   "metadata": {},
   "source": [
    "#### Missing values\n",
    "\n",
    "Dealing with missing values: The Titanic dataset has missing values in the Age, Cabin, and Embarked columns. One approach to handling missing values is to impute the missing values with mean or median values. Alternatively, rows with missing values can be dropped from the dataset.\n",
    "\n",
    "Imputation of missing values can potentially cause a lot of bias, and you should be careful thinking about the underlying reasons for data being missing and the potential bias caused by imputation.\n",
    "\n",
    "You can read more in this reference: [GarcÃ­a, S., Luengo, J., & Herrera, F. (2015). Data preprocessing in data mining.](https://d1wqtxts1xzle7.cloudfront.net/60477900/Garcia__Luengo__Herrera-Data_Preprocessing_in_Data_Mining_-_Springer_International_Publishing_201520190903-77973-th1o73-libre.pdf?1567544443=&response-content-disposition=inline%3B+filename%3DIntelligent_Systems_Reference_Library_72.pdf&Expires=1679794027&Signature=Tbl8YhiUQworlYTbuS6GmJdj94mgY2vfpY86Tk7cVQEgk4qXV9~bjXxEjJWZgYxGEp724F2KkJU-WM9euX46J0d-6OlQBekLA8o7GcJ0SUNoXrE2gNzbr5SExsKeMqAYfBtmZVzlwkWTgL7WCha7lXhtPJmnmTMYl0wRiV1QA4MuAZUN-lliWU9SKdut48~KCDXRQ-sybHdakWoEL7Q1nq4JTXxreu~eMs996UJqylo0dftBtab6AGENHCw3FKUSi6CnekNrOV6fGISRIS1vcZaZdeZlfr5ywHjaQIGvobWS0--k6KtS9wVvl-28RZKbzrp2AbAqw2slXmrE-ADjbA__&Key-Pair-Id=APKAJLOHF5GGSLRBV4ZA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e578abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = titanic.iloc[[40,358,789]]\n",
    "titanicdub = pd.concat([titanic, x])\n",
    "titanicdub.info()\n",
    "titanicdub.to_csv('data/titanic_with_dub.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b21d405d",
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8115e543",
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic.isna().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e3d104",
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic[titanic.Embarked.isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2280b8f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic[titanic.Cabin.isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "805d519a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(titanic.Pclass, titanic.Cabin.isna())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cdf31c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic[titanic.Age.isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a985dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic.groupby('Pclass').Age.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "199905f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.scatter(titanic['Fare'], titanic['Age'], c=titanic['Pclass'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1458ff23",
   "metadata": {},
   "source": [
    "We will impute missing Age using linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e3cca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "lr = LinearRegression()\n",
    "columns = ['Pclass']\n",
    "titanic.head()\n",
    "\n",
    "testdf = titanic.loc[titanic['Age'].isnull()]\n",
    "traindf = titanic.loc[titanic['Age'].isnull()==False]\n",
    "\n",
    "lr.fit(traindf[columns],traindf['Age'])\n",
    "\n",
    "pred = lr.predict(testdf[columns])\n",
    "testdf['Age']= pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b89153cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic = pd.concat([testdf, traindf], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eabcf5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e98f8c0",
   "metadata": {},
   "source": [
    "#### Removing dublicates\n",
    "Dealing with duplicates: Check for duplicates in the dataset, which can arise due to data entry errors or data collection methods. Duplicates can be dropped or the data can be aggregated to remove duplicate values.\n",
    "\n",
    "There are no duplicates in the titanic data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ff08a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic[titanic.duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e72f84d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop duplicate rows\n",
    "titanic.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f3c49b0",
   "metadata": {},
   "source": [
    "#### Correct inconsistent data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b3f3b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b14475",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Sex column to binary variable\n",
    "titanic[\"Sex\"] = pd.get_dummies(titanic[\"Sex\"]).male"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88da2f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eae7ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8992002d",
   "metadata": {},
   "source": [
    "#### Handle outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59fb6cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt # pip install matplotlib\n",
    "\n",
    "# Create boxplots to visualize outliers in numerical variables\n",
    "fig, axs = plt.subplots(1, 2, figsize=(10,5))\n",
    "axs[0].boxplot(titanic[\"Age\"])\n",
    "axs[0].set_title(\"Boxplot of Age\")\n",
    "axs[1].boxplot(titanic[\"Fare\"])\n",
    "axs[1].set_title(\"Boxplot of Fare\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "039cbc99",
   "metadata": {},
   "source": [
    "*A boxplot from the Matplotlib library shows the median value as a horizontal line inside a box that represents the interquartile range (IQR), with the lower and upper whiskers indicating the lowest and highest non-outlier values within 1.5 times the IQR of the lower and upper quartile, respectively. Outliers are displayed as individual points outside the whiskers.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "665e3329",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate z-scores to identify outliers in numerical variables\n",
    "from scipy import stats\n",
    "z_scores_age = stats.zscore(titanic[\"Age\"])\n",
    "z_scores_fare = stats.zscore(titanic[\"Fare\"])\n",
    "threshold = 3\n",
    "outliers_age = titanic[\"Age\"][abs(z_scores_age) > threshold]\n",
    "outliers_fare = titanic[\"Fare\"][abs(z_scores_fare) > threshold]\n",
    "print(\"Outliers in Age:\", outliers_age)\n",
    "print(\"Outliers in Fare:\", outliers_fare)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75fe8d93",
   "metadata": {},
   "source": [
    "#### Standardize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0dbf16a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale numerical variables\n",
    "from sklearn.preprocessing import StandardScaler, MaxAbsScaler\n",
    "scaler = MaxAbsScaler()\n",
    "titanic[[\"Age\", \"Fare\"]] = scaler.fit_transform(titanic[[\"Age\", \"Fare\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34e11dc5",
   "metadata": {},
   "source": [
    "return to [overview](../00_overview.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
