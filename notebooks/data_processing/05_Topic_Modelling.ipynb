{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will be exploring the unsupervised technique __topic modelling__, but let's first load a toy dataset used in the paper: \n",
    "\n",
    "> Xiang Zhang, Junbo Zhao, Yann LeCun. Character-level Convolutional Networks for Text Classification. Advances in Neural Information Processing Systems 28 (NIPS 2015).\n",
    "\n",
    "This is actually the testing set (for algorithm evaluation) but we will just use this as an illustration for text analytics procedures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-03T19:02:34.663292Z",
     "start_time": "2018-04-03T19:02:34.249602Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-03T19:02:35.089928Z",
     "start_time": "2018-04-03T19:02:35.034226Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/ag_news.csv')\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are four columns: \n",
    "- class index and class names that annotate the content type\n",
    "- title and description of each news piece"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's combine the title and description into a column called `content` and drop unneccessary columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-03T19:03:21.954463Z",
     "start_time": "2018-04-03T19:03:21.932703Z"
    }
   },
   "outputs": [],
   "source": [
    "df['content'] = df['title'] + '. ' + df['description']\n",
    "df.drop(columns=['title', 'description'], inplace=True)\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### bag-of-words\n",
    "We'll first create a bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from string import punctuation\n",
    "from nltk.corpus import stopwords\n",
    "eng_stopwords = stopwords.words('english')\n",
    "from nltk import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def tokenize(df, min_length=3):\n",
    "    \"\"\"\n",
    "    Tokenize the text content of a DataFrame.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas.DataFrame\n",
    "        A DataFrame with a 'content' column containing the text to be tokenized.\n",
    "    min_length : int, optional\n",
    "        The minimum length of a token to include in the output. Default is 3.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list\n",
    "        A list of tokenized documents, where each document is represented as a list of tokens.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    This function uses the NLTK library for tokenization, removes punctuation and stopwords, and applies stemming.\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> import pandas as pd\n",
    "    >>> df = pd.DataFrame({'content': ['This is a test.', 'Another test sentence.'], 'label': [0, 1]})\n",
    "    >>> tokens = tokenize(df)\n",
    "    >>> tokens\n",
    "    [['test'], ['anoth', 'test', 'sentenc']]\n",
    "    \"\"\"\n",
    "    bow = [nltk.word_tokenize(content.lower()) for content in df['content'].values]\n",
    "    bow = [[w for w in d if w not in punctuation and w not in eng_stopwords and not w.isdigit()] for d in bow]\n",
    "    trans = str.maketrans('', '', punctuation)\n",
    "    bow = [[w.translate(trans).strip() for w in d] for d in bow]\n",
    "    bow = [[w for w in d if len(w) >= min_length] for d in bow]\n",
    "    bow = [[stemmer.stem(w) for w in d] for d in bow]\n",
    "    \n",
    "    return bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow = tokenize(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Vector Space Model (VSM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we have a bag of words, we can create vectors based on these items. Instead of using tokens/text, it is sometimes easier to just use integer indices. For example, `race` is the first word and therefore the number `1` maps to `race`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this task, I like to use [`gensim`](https://radimrehurek.com/gensim/index.html), which has a library of very well written and convenient APIs, especially for [topic modeling](https://en.wikipedia.org/wiki/Topic_model) and [word2vec](https://rare-technologies.com/word2vec-tutorial/) algorithms:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`pip install gensim`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-03T19:25:41.068099Z",
     "start_time": "2018-04-03T19:25:40.120687Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "dictionary = gensim.corpora.Dictionary(bow)\n",
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-03T19:26:14.668636Z",
     "start_time": "2018-04-03T19:26:14.638436Z"
    }
   },
   "outputs": [],
   "source": [
    "dictionaryl.token2id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mapping of tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-03T19:26:29.081726Z",
     "start_time": "2018-04-03T19:26:29.076804Z"
    }
   },
   "outputs": [],
   "source": [
    "dictionary.token2id['disappoint'], dictionary.token2id['california']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upon creation of a dictionary that maps words to integers (and vice versa), we can transform our bag of words. Each document will be a list of tuples that contain token indices and frequencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-03T19:27:02.065526Z",
     "start_time": "2018-04-03T19:27:01.845541Z"
    }
   },
   "outputs": [],
   "source": [
    "corpus = [dictionary.doc2bow(d) for d in bow]\n",
    "corpus[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's investigate e.g. the token with the highest frequency (11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary.id2token[11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Topic Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Latent Dirichet Allocation (LDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A very commonly used dimensionality reduction technique family is called ___topic modeling___. It assumes that each document is a mixture of topics, where each topic is a mixture of terms. One of the most successful algorithms is [___latent Dirichlet allocation___](https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation) (LDA), whose corresponding paper is:\n",
    "> Blei, D. M., Ng, A. Y., & Jordan, M. I. (2003). Latent dirichlet allocation. Journal of machine Learning research, 3(Jan), 993-1022."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LDA is a generative model that does the reverse engineerging of document generation. It can be represented as a probablistic graphical model:\n",
    "![lda](https://upload.wikimedia.org/wikipedia/commons/4/4d/Smoothed_LDA.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The generative process can be described as follows:\n",
    "- For each topic $k$, sample a multinomial distribution $\\phi_k$ over words from the Dirichlet prior with parameter $\\beta$\n",
    "- For each document $m$, sample a multinomial distribution $\\theta_m$ over topics from the Dirichlet prior with parameter $\\alpha$\n",
    "    - For each word $n$ in $m$:\n",
    "        - Sample a topic $z_{m,n}$ from the correponding topic distribution parameterized by $\\theta_m$\n",
    "        - Sample a word $w_{m,n}$ from the correponding topic $z_{m,n}$'s word distribution parameterized by $\\phi_{z_{m,n}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Parameters in LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generally, we need to control two hyperparamters of a LDA model:\n",
    "- Topic-word Dirichlet prior $\\beta$\n",
    "- Document-topic Dirichlet prior $\\alpha$\n",
    "\n",
    "The selection of these parameters are application dependent. Heuristically, people will choose $\\alpha=\\dfrac{50}{K}$ and $\\beta=0.01$, as described in\n",
    "\n",
    "> Griffiths, T. L., and Steyvers, M. 2004. “Finding Scientific Topics,” Proceedings of the National Academy of Sciences (101:Supplement 1), National Academy of Sciences, pp. 5228–5235.\n",
    "\n",
    "It is also possible to infer these two hyperparameters given the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The selection of $K$ totally depends on the context. It is also possible to select a topic number based on quantitative measures of topic modeling quality, but this is beyond the scope of this tutorial.\n",
    "\n",
    "For our toy sample set, we will just select $K=4$ because there are 4 labels: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-03T19:42:53.362356Z",
     "start_time": "2018-04-03T19:42:53.356128Z"
    }
   },
   "outputs": [],
   "source": [
    "df.class_name.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Run LDA!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thanks to the convenient APIs by `gensim`, we can easily run [LDA in Python](https://radimrehurek.com/gensim/models/ldamodel.html):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-03T19:46:13.129939Z",
     "start_time": "2018-04-03T19:44:39.239135Z"
    }
   },
   "outputs": [],
   "source": [
    "from gensim.models import LdaModel\n",
    "lda = LdaModel(corpus, num_topics=4, id2word=dictionary, passes=10, \n",
    "               minimum_probability=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Analysis on LDA results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the output of LDA. First, we can check if the topics make sense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-03T19:46:45.406370Z",
     "start_time": "2018-04-03T19:46:45.398364Z"
    }
   },
   "outputs": [],
   "source": [
    "for _, topic_str in lda.show_topics():\n",
    "    print(topic_str)\n",
    "    print('------------'*10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While we probably cannot say the topics are perfect, they are okay. We can interpret the topics as: sci/tech, sports, world, and business."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each document, we can check their topic distributions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-03T19:49:19.069553Z",
     "start_time": "2018-04-03T19:49:19.061953Z"
    }
   },
   "outputs": [],
   "source": [
    "i = 1000\n",
    "lda.get_document_topics(corpus[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that topic 0, which is interpreted \"business\" topics dominate this document. We can check to see if this makes sense:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-03T19:50:15.220687Z",
     "start_time": "2018-04-03T19:50:15.202603Z"
    }
   },
   "outputs": [],
   "source": [
    "df.loc[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In fact, LDA can be used in many situtaions, such as information retrieval, document clustering and labeling, and even for images! Here we just mention the simplest use case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "return to [overview](../00_overview.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
